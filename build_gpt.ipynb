{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cells below were created as notes while watching Andrej Karpathy's video \"[Let's build GPT: from scratch, in code, spelled out.\n",
    "](https://www.youtube.com/watch?v=kCc8FmEb1nY)\". Check it out if you haven't!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens - encoding text to a numeric format\n",
    "\n",
    "Tokens are numerical representations of the smallest meaningful units of text in natural language. They can represent words, subwords, characters or even symbols.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's look at the tokens used in GPT-2:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathanb/git/nlp-refresher/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', ' string', ' to', ' demonstrate', ' the', ' token', 'izer', '.']\n",
      "[32, 4731, 284, 10176, 262, 11241, 7509, 13]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# This is the tokenizer used by GPT-2.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "test_str = \"A string to demonstrate the tokenizer.\"\n",
    "\n",
    "# GPT-2 used a subword tokenizer, meaning that each token corresponds to part of a word\n",
    "str_enc = tokenizer.encode(test_str)  # Tokenized string\n",
    "print([tokenizer.decode([s]) for s in str_enc])\n",
    "print(str_enc)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the text was split into pieces corresponding to a combination of words, subwords and special characters and converted into tokens in the form of integers.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set of available tokens is called the _vocabulary_.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' refreshed', 'Enc', ' accommodations', ' explicitly', ' Plex', ' glyc', ' flies', 'POS', 'abilia', 'icates', ' Beta', ' Buk', ' Race', '10', 'ARC', 'NET', 'usterity', ' Vote', ' Ru', ' bout']\n",
      "Vocab size: 50257\n"
     ]
    }
   ],
   "source": [
    "from random import sample\n",
    "\n",
    "# Let's decode a few tokens back to text and print them...\n",
    "tokens = tokenizer.get_vocab().keys()\n",
    "print([tokenizer.convert_tokens_to_string([t]) for t in sample(tokens, 20)])\n",
    "\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 6\n",
      "Length of encoded seq: 8\n"
     ]
    }
   ],
   "source": [
    "# For this tokenizer, tokens correspond to subwords. Because of this, the encoded sequence is longer than the amount of words in the text.\n",
    "print(f\"Length of text: {len(test_str.split(' '))}\")\n",
    "print(f\"Length of encoded seq: {len(str_enc)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will be using text from Shakespeare as our corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:300])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a very simple tokenization scheme - encoding single characters as tokens.\n",
    "\n",
    "Therefore, our vocabulary will consist of all symbols used in the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "print(\"\".join(vocab))\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size: {vocab_size}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our vocabulary is much smaller compared to the GPT-2 tokenizer. Keep this in mind as we continue!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple tokenization scheme by using the character's index in the vocabulary as its token.\n",
    "stoi = {ch: i for i, ch in enumerate(vocab)}  # string-to-integer\n",
    "itos = {i: ch for i, ch in enumerate(vocab)}  # integer-to-string\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([itos[i] for i in l])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's encode our original example text again, now using this simple tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 1, 57, 58, 56, 47, 52, 45, 1, 58, 53, 1, 42, 43, 51, 53, 52, 57, 58, 56, 39, 58, 43, 1, 58, 46, 43, 1, 58, 53, 49, 43, 52, 47, 64, 43, 56, 8]\n",
      "A string to demonstrate the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "print(encode(test_str))\n",
    "print(decode(encode(test_str)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how our encoded sequence compares to the original text in length now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 6\n",
      "Length of encoded seq: 38\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of text: {len(test_str.split(' '))}\")\n",
    "print(f\"Length of encoded seq: {len(encode(test_str))}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how it is much longer? Since we are using a smaller vocabulary, we must use more tokens to encode our sequences.\n",
    "\n",
    "This shows the inherent relationship between vocabulary size and sequence length - a smaller vocabulary results in longer sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 16\n",
    "train_data[: block_size + 1]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram model\n",
    "\n",
    "Let's build arguably the simplest language model possible - a bigram model, which predicts the next token based on the previous token only. This means that we are modeling our text as Markov process, where the probability of the next state (token) only depends on the present state (the previous token),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.embedding(idx)\n",
    "\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits_ = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits_, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, num_steps):\n",
    "        for _ in range(num_steps):\n",
    "            logits, _ = self.forward(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            # Note: Sampling is probabilistic.\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 4.890367031097412\n",
      "Train loss: 3.279221773147583\n",
      "Train loss: 2.6370930671691895\n",
      "Train loss: 2.79683518409729\n",
      "Train loss: 2.7508437633514404\n",
      "Train loss: 2.8681375980377197\n",
      "Train loss: 2.367644786834717\n",
      "Train loss: 2.5374605655670166\n",
      "Train loss: 2.2663519382476807\n",
      "Train loss: 2.4319400787353516\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "for iter in range(10000):\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if iter % 1000 == 0:\n",
    "        print(f\"Train loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gopsote\n",
      "\n",
      "\n",
      "NURDWhiger, mbe cat\n",
      "\n",
      "\n",
      "Ane hau,\n",
      "\n",
      "\n",
      "DWers hin, toke ftop\n",
      "\n",
      "\n",
      "Me phemasther henamis adsh toshed as mashes. s,\n",
      "Fas being bld toureconat y y:\n",
      "Po t m hers,\n",
      "ak I the t fthyo l,\n",
      "\n",
      "\n",
      "Mur:\n",
      "DI ge dutow ffriorptanecablir:\n",
      "\n",
      "Wing l haspouryond set moned d, pestory I inooy ff me.\n",
      "E t! pound,\n",
      "If akne takis st,\n",
      "SALICAnd blis bices. thand marorkenof onld sth plarerdoreawer beendvieano al nel tre h thave gsinot.\n",
      "RYor nd,\n",
      "CEThand, yofowncoto d st with heathige's moll co wrkirs w an ot\n",
      "APlors,\n",
      "Hars s'shig ed grk Youccelale y, n hime way'dingh inoigackean:\n",
      "\n",
      "INCE wave houchanghy horind pay tshe oron an, f teldat tis h y pove.\n",
      "\n",
      "CENI t, hucoks meade hater ches omu d biowr, can! movins.\n",
      "RLo thid wn m art tle tople he.\n",
      "Whe nd.\n",
      "Berursteriknorveviu hel cus s hofered t bise-he my d ange\n",
      "pe, ordere RAwit yod'liokier st, inse, h ad, the PELOut LINO wn youdiers.\n",
      "IO:\n",
      "Dulo, herd st orer s whe amarent!\n",
      "Y:\n",
      "St, t yowit clein th, ay wirdowit msth, ucar tonerooie m sle t, most beis,\n",
      "BEENGHe lld lono hthid ine son! who s\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(torch.zeros((1, 1), dtype=torch.long), 1000)\n",
    "\n",
    "print(decode(output[0].tolist()))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not that bad, but far from perfect.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive way to predict next token is by using the mean of all previous token's embeddings.\n",
    "\n",
    "$$ \\hat{x*t} = f(\\Sigma*{i=0}^{t-1} x_i) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 4])\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [4., 4., 4., 4.],\n",
      "        [5., 5., 5., 5.],\n",
      "        [6., 6., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "T, C = 6, 4\n",
    "\n",
    "# Let's create a mock sequence of embedded tokens. Let's give each row incrementing values to more easily see what happens.\n",
    "x = torch.arange(1, T + 1).view(-1, 1).repeat(1, C).float()\n",
    "\n",
    "print(x.shape)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.5000, 1.5000, 1.5000, 1.5000],\n",
      "        [2.0000, 2.0000, 2.0000, 2.0000],\n",
      "        [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "        [3.0000, 3.0000, 3.0000, 3.0000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000]])\n"
     ]
    }
   ],
   "source": [
    "# Slow way, for-loop...\n",
    "\n",
    "\n",
    "def mean(x):\n",
    "    xbow = torch.zeros((T, C))\n",
    "\n",
    "    for t in range(T):  # For each time step in seq.\n",
    "        xprev = x[: t + 1, :]\n",
    "        xbow[t] = torch.mean(xprev, dim=0)\n",
    "\n",
    "    return xbow\n",
    "\n",
    "\n",
    "print(mean(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Fast way - matrix mult!\n",
    "\n",
    "wei = torch.tril(torch.ones(T, T))  # Create lower triangular matrix.\n",
    "print(wei)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.,  1.],\n",
      "        [ 3.,  3.,  3.,  3.],\n",
      "        [ 6.,  6.,  6.,  6.],\n",
      "        [10., 10., 10., 10.],\n",
      "        [15., 15., 15., 15.],\n",
      "        [21., 21., 21., 21.]])\n"
     ]
    }
   ],
   "source": [
    "# When matmuled with sequence with shape (T, C), we almost get what we want - we get the sum of the rows, but not the mean.\n",
    "print(wei @ x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.5000, 1.5000, 1.5000, 1.5000],\n",
      "        [2.0000, 2.0000, 2.0000, 2.0000],\n",
      "        [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "        [3.0000, 3.0000, 3.0000, 3.0000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000]])\n"
     ]
    }
   ],
   "source": [
    "# To get the mean instead, we normalize each row to sum to 1.\n",
    "row_norm = wei.sum(dim=1, keepdim=True)\n",
    "wei = wei / row_norm\n",
    "\n",
    "print(wei @ x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare speeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.03 µs ± 81.4 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "wei @ x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.8 µs ± 5.08 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "mean(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than 10x faster, and we are not even using the GPU - not bad\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there is one more way to achieve the same outcome using softmax across each row, which normalizes it to sum to 1. To mask out future tokens, we replace their values by -inf first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]])\n",
      "tensor([[1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.5000, 1.5000, 1.5000, 1.5000],\n",
      "        [2.0000, 2.0000, 2.0000, 2.0000],\n",
      "        [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "        [3.0000, 3.0000, 3.0000, 3.0000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000]])\n"
     ]
    }
   ],
   "source": [
    "tri = torch.tril(torch.ones(T, T))\n",
    "\n",
    "wei = torch.ones((T, T))\n",
    "wei = wei.masked_fill(tri == 0, -torch.inf)\n",
    "wei = F.softmax(wei, dim=1)\n",
    "\n",
    "print(wei)\n",
    "print(wei @ x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason this is interesting is because this approach works regardless of the initial values of the weights matrix.\n",
    "\n",
    "We can fill it with random values and still get rows that are normalized to 1:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.7089, 0.2911, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5166, 0.3210, 0.1624, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1616, 0.3187, 0.1423, 0.3774, 0.0000, 0.0000],\n",
       "        [0.3024, 0.1149, 0.0874, 0.4666, 0.0287, 0.0000],\n",
       "        [0.1292, 0.1173, 0.1132, 0.0494, 0.1582, 0.4326]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.randn((T, T))\n",
    "wei = wei.masked_fill(tri == 0, -torch.inf)\n",
    "wei = F.softmax(wei, dim=1)\n",
    "\n",
    "wei\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Describe how this can be intepreted as affinities, or coupling, between tokens.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-attention\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, can we do better than a simple average of the previous tokens? Indeed we can! We can do a weighted sum, where the weights represent the importance of each token. Even better, we can do a _learned_ weighted sum, where the weights are data-dependent.\n",
    "\n",
    "This is in fact exactly what the mechanism underlying transformers does, so called _self-attention_. It works like this:\n",
    "\n",
    "1. Each token emits three values: 1) a query _Q_, telling other tokens what it is \"looking for\", 2) a key _K_, telling other tokens its own identity, and 3) a value _V_, representing the token in the weighted sum. These are all outputs of learned functions, typically `nn.Linear` layers.\n",
    "2. Every token's query is multiplied through a dot-product with every other token's key, giving the _attention weights_ $\\alpha$.\n",
    "3. The attention weights are used to do a weighted sum of the values to produce the output features.\n",
    "\n",
    "The image below shows a single token's query interacting with the keys and values of the other tokens:\n",
    "\n",
    "![Alt text](./images/attention_example.svg)\n",
    "\n",
    "Remember that for a generative language model like GPT, we only want a given token $x_t$ to interact with the tokens that came before, hence $\\alpha_{i>t} = 0$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like mentioned above, the queries, keys and values are outputs of `nn.Linear` layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [5.9165e-01, 4.0835e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [3.1789e-01, 6.5987e-02, 6.1612e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [4.5868e-06, 1.2830e-02, 9.8440e-01, 2.7620e-03, 0.0000e+00, 0.0000e+00],\n",
      "        [9.9107e-01, 2.0629e-03, 4.1371e-04, 3.6758e-04, 6.0813e-03, 0.0000e+00],\n",
      "        [9.7590e-01, 6.9176e-03, 1.0155e-03, 5.5064e-03, 2.5252e-03, 8.1350e-03]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-0.7291,  2.2285,  2.4719,  1.1753,  1.3283, -2.6645,  0.1657, -1.7303,\n",
      "         -1.5886, -1.9180,  0.2320, -1.8639,  0.8100,  2.0086,  1.2171,  0.0767,\n",
      "          1.2085, -1.0426, -2.1063, -0.2117,  0.6198, -2.5657, -3.2720, -2.2933,\n",
      "         -0.4154,  1.4644,  1.3094, -0.0176, -0.9738, -0.8888, -0.6673,  1.0015],\n",
      "        [-0.2837,  1.2334,  1.3860,  0.5857,  0.4858, -1.5227,  0.0211, -1.1075,\n",
      "         -0.8909, -1.2555,  0.1499, -1.0270,  0.5117,  1.2732,  0.5861, -0.1996,\n",
      "          0.7223, -0.4971, -1.3820, -0.3465,  0.3777, -1.7100, -2.0466, -1.5502,\n",
      "         -0.3690,  0.8078,  0.8862,  0.2106, -0.6518, -0.6348, -0.5281,  0.5090],\n",
      "        [ 0.2977,  0.1876,  0.0726,  0.2081, -0.0528, -0.2949, -0.3865, -0.3009,\n",
      "          0.0694, -0.6909, -0.3023, -0.1058,  0.1080,  0.6394, -0.1212, -0.1938,\n",
      "          0.5157,  0.0217, -0.6042, -0.2304,  0.2166, -0.4856, -0.8999, -0.6846,\n",
      "         -0.2085, -0.0767,  0.1212,  0.3455, -0.5194, -0.4361, -0.5517,  0.0694],\n",
      "        [ 0.8136, -0.8152, -1.1234, -0.2430, -0.6968,  0.8724, -0.6831,  0.4165,\n",
      "          0.9066, -0.1011, -0.6012,  0.7623, -0.2458, -0.0197, -0.7830, -0.2975,\n",
      "          0.2056,  0.5392,  0.1355, -0.2139,  0.0274,  0.5686,  0.2517,  0.1143,\n",
      "         -0.0960, -0.8533, -0.4943,  0.5132, -0.3165, -0.2205, -0.5116, -0.3810],\n",
      "        [-0.7193,  2.2076,  2.4455,  1.1656,  1.3139, -2.6418,  0.1576, -1.7133,\n",
      "         -1.5696, -1.9052,  0.2245, -1.8465,  0.8013,  1.9969,  1.2019,  0.0764,\n",
      "          1.2007, -1.0308, -2.0901, -0.2102,  0.6190, -2.5406, -3.2482, -2.2753,\n",
      "         -0.4097,  1.4474,  1.2933, -0.0105, -0.9713, -0.8849, -0.6690,  0.9938],\n",
      "        [-0.7018,  2.1665,  2.4056,  1.1361,  1.2741, -2.5974,  0.1545, -1.6915,\n",
      "         -1.5452, -1.8753,  0.2329, -1.8130,  0.7953,  1.9600,  1.1795,  0.0571,\n",
      "          1.1772, -1.0091, -2.0616, -0.2245,  0.6070, -2.5168, -3.1942, -2.2472,\n",
      "         -0.4157,  1.4233,  1.2855, -0.0033, -0.9549, -0.8756, -0.6546,  0.9701]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "head_size = 32\n",
    "\n",
    "x = torch.randn(T, C)\n",
    "\n",
    "key = nn.Linear(C, head_size)\n",
    "query = nn.Linear(C, head_size)\n",
    "value = nn.Linear(C, head_size)\n",
    "\n",
    "k = key(x)  # (T, head_size)\n",
    "q = query(x)  # (T, head_size)\n",
    "v = value(x)  # (T, head_size)\n",
    "\n",
    "wei = q @ k.T  # (T, head_size) @ (head_size, T) = (T, T)\n",
    "wei = wei.masked_fill(tri == 0, -torch.inf)\n",
    "wei = F.softmax(wei, dim=1)\n",
    "\n",
    "print(wei)\n",
    "print(wei @ v)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original transformer paper used a particular implementation of self-attention called _scaled dot-product attention_.\n",
    "\n",
    "$$\n",
    "Attention(Q, K, V) = softmax\\biggl(\\frac{Q K^T}{\\sqrt{d}}V\\biggr)\n",
    "$$\n",
    "\n",
    "<img src=\"./images/scaled_dot_product_attn.svg\" width=\"256\" style=\"margin:auto\"/>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one piece still missing - the scaling by $1/\\sqrt{d_K}$. This is done in order to ensure a that `wei` has unit variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0999)\n",
      "tensor(0.9475)\n",
      "tensor(28.0048)\n",
      "tensor(0.8751)\n"
     ]
    }
   ],
   "source": [
    "k = torch.randn(T, head_size)\n",
    "q = torch.randn(T, head_size)\n",
    "wei = q @ k.T\n",
    "wei_norm = wei * 1 / head_size**0.5\n",
    "\n",
    "print(k.var())\n",
    "print(q.var())\n",
    "print(wei.var())\n",
    "print(wei_norm.var())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without this normalization, `wei` might not be sufficiently uniform and result in a spiky distribution when fed into the softmax function. This would cause tokens to almost only interact with single other tokens, slowing down learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = F.softmax(wei.masked_fill(tri == 0, -torch.inf), dim=1)\n",
    "attn_norm = F.softmax(wei_norm.masked_fill(tri == 0, -torch.inf), dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, layout=\"constrained\")\n",
    "axs[0].imshow(attn)\n",
    "axs[1].imshow(attn_norm)\n",
    "axs[0].title.set_text(\"Without normalization\")\n",
    "axs[1].title.set_text(\"With normalization\")\n",
    "fig.suptitle(\"Attention weight matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, emb_size, head_size, block_size) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_size = head_size\n",
    "\n",
    "        self.query = nn.Linear(emb_size, head_size, bias=False)\n",
    "        self.key = nn.Linear(emb_size, head_size, bias=False)\n",
    "        self.value = nn.Linear(emb_size, head_size, bias=False)\n",
    "\n",
    "        self.register_buffer(\"tri\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        q = self.query(x)  # (B, T, head_size)\n",
    "        k = self.key(x)  # (B, T, head_size)\n",
    "        v = self.value(x)  # (B, T, head_size)\n",
    "\n",
    "        wei = q @ k.transpose(-2, -1)  # (B, T, head_size) @ (B, head_size, T) = (B, T, T)\n",
    "        wei = wei.masked_fill(self.tri[:T, :T] == 0, -torch.inf)\n",
    "        wei = F.softmax(wei, dim=1)\n",
    "\n",
    "        return wei @ v  # (B, T, T) @ (B, T, head_size) = (B, T, head_size)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention is typically combined with other layers to form a transformer block. Here is an example of a simple such block: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_size, head_size, num_heads=4, block_size=32) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList(ScaledDotProductAttention(emb_size, head_size, block_size) for _ in range(num_heads))\n",
    "        self.mlp = nn.Linear(head_size * num_heads, emb_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb_cat = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        out = self.mlp(emb_cat)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple transformer blocks can be stacked on top of each other to form a deeper network:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.0784e-01,  1.1336e-03],\n",
      "         [ 5.7909e-01,  7.5903e-01],\n",
      "         [ 4.5702e-01, -5.4675e-02],\n",
      "         [ 1.8800e+00, -1.3266e+00]]])\n",
      "tensor([[[ 0.0477,  0.0864],\n",
      "         [-0.0454,  0.0400],\n",
      "         [-0.0192,  0.0496],\n",
      "         [-0.2259,  0.0458]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.0960, -0.0463],\n",
      "         [ 0.0965, -0.0469],\n",
      "         [ 0.1010, -0.0480],\n",
      "         [ 0.0693, -0.0522]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.0270, -0.0684],\n",
      "         [ 0.0125, -0.0672],\n",
      "         [-0.0103, -0.0650],\n",
      "         [-0.0358, -0.0682]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.0559, -0.0597],\n",
      "         [-0.0505, -0.0669],\n",
      "         [-0.0436, -0.0733],\n",
      "         [-0.0315, -0.0776]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "B, T, C = 1, 4, 2\n",
    "x0 = torch.randn(B, T, C)\n",
    "\n",
    "num_blocks = 4\n",
    "blocks = [TransformerBlock(C, head_size=32, num_heads=4, block_size=32) for _ in range(num_blocks)]\n",
    "\n",
    "xs = [x0]\n",
    "for block in blocks:\n",
    "    x = block(xs[-1])\n",
    "    xs.append(x)\n",
    "\n",
    "print(\"\\n\".join([str(x) for x in xs]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
