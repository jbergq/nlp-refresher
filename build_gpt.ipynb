{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cells below were created as notes while watching Andrej Karpathy's video \"[Let's build GPT: from scratch, in code, spelled out.\n",
    "](https://www.youtube.com/watch?v=kCc8FmEb1nY)\". Check it out if you haven't!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens - encoding text to a numeric format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathanb/git/nlp-refresher/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 50257\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# This is the tokenizer used by GPT-2.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', ' rather', ' long', ' text', ' to', ' demonstrate', ' the', ' token', 'izer', '.', ' Co', 'ool', ',', ' right', '?']\n",
      "[32, 2138, 890, 2420, 284, 10176, 262, 11241, 7509, 13, 1766, 970, 11, 826, 30]\n"
     ]
    }
   ],
   "source": [
    "test_str = \"A rather long text to demonstrate the tokenizer. Coool, right?\"\n",
    "\n",
    "# GPT-2 used a subword tokenizer, meaning that each token corresponds to part of a word\n",
    "str_enc = tokenizer.encode(test_str)  # Tokenized string\n",
    "print([tokenizer.decode([s]) for s in str_enc])\n",
    "print(str_enc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 10\n",
      "Length of encoded seq: 15\n"
     ]
    }
   ],
   "source": [
    "# Note that tokens correspond to subwords. Because of this, the encoded sequence has more tokens compared to the number of words in the encoded text.\n",
    "print(f\"Length of text: {len(test_str.split(' '))}\")\n",
    "print(f\"Length of encoded seq: {len(str_enc)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will be using text from Shakespeare as our corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a very simple tokenization scheme - encoding single characters as tokens.\n",
    "\n",
    "Therefore, our vocabulary will consist of all symbols used in the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "print(\"\".join(vocab))\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size: {vocab_size}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our vocabulary is much smaller compared to the GPT-2 tokenizer. Keep this in mind as we continue!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple tokenization scheme by using the character's index in the vocabulary as its token.\n",
    "stoi = {ch: i for i, ch in enumerate(vocab)}  # string-to-integer\n",
    "itos = {i: ch for i, ch in enumerate(vocab)}  # integer-to-string\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([itos[i] for i in l])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's encode our original example text again, now using this simple tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 1, 56, 39, 58, 46, 43, 56, 1, 50, 53, 52, 45, 1, 58, 43, 62, 58, 1, 58, 53, 1, 42, 43, 51, 53, 52, 57, 58, 56, 39, 58, 43, 1, 58, 46, 43, 1, 58, 53, 49, 43, 52, 47, 64, 43, 56, 8, 1, 15, 53, 53, 53, 50, 6, 1, 56, 47, 45, 46, 58, 12]\n",
      "A rather long text to demonstrate the tokenizer. Coool, right?\n"
     ]
    }
   ],
   "source": [
    "print(encode(test_str))\n",
    "print(decode(encode(test_str)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how our encoded sequence compares to the original text in length now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 10\n",
      "Length of encoded seq: 62\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of text: {len(test_str.split(' '))}\")\n",
    "print(f\"Length of encoded seq: {len(encode(test_str))}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how it is much longer? Since we are using a smaller vocabulary, we must use more tokens to encode our sequences.\n",
    "\n",
    "This shows the inherent relationship between vocabulary size and sequence length - a smaller vocabulary results in longer sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[: block_size + 1]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram model - Predicting based on the previous token only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.embedding(idx)\n",
    "\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits_ = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits_, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, num_steps):\n",
    "        for _ in range(num_steps):\n",
    "            logits, _ = self.forward(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            # Note: Sampling is probabilistic.\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 2.195941925048828\n",
      "Train loss: 2.5153584480285645\n",
      "Train loss: 2.5030364990234375\n",
      "Train loss: 2.493042469024658\n",
      "Train loss: 2.611518144607544\n",
      "Train loss: 2.4348185062408447\n",
      "Train loss: 2.168931722640991\n",
      "Train loss: 2.555227518081665\n",
      "Train loss: 2.0753352642059326\n",
      "Train loss: 2.399522304534912\n",
      "Train loss: 2.3705391883850098\n",
      "Train loss: 2.786844253540039\n",
      "Train loss: 2.861818552017212\n",
      "Train loss: 3.1184353828430176\n",
      "Train loss: 2.8109734058380127\n",
      "Train loss: 2.684131622314453\n",
      "Train loss: 3.411339521408081\n",
      "Train loss: 2.2716832160949707\n",
      "Train loss: 2.417025327682495\n",
      "Train loss: 2.802736520767212\n",
      "Train loss: 3.864915132522583\n",
      "Train loss: 1.9487805366516113\n",
      "Train loss: 2.4493465423583984\n",
      "Train loss: 2.2473337650299072\n",
      "Train loss: 2.306190252304077\n",
      "Train loss: 2.722623109817505\n",
      "Train loss: 2.0977742671966553\n",
      "Train loss: 2.344184637069702\n",
      "Train loss: 2.1484503746032715\n",
      "Train loss: 3.2914156913757324\n",
      "Train loss: 3.233902931213379\n",
      "Train loss: 2.4903721809387207\n",
      "Train loss: 2.417214870452881\n",
      "Train loss: 2.4159724712371826\n",
      "Train loss: 2.3700449466705322\n",
      "Train loss: 2.644664764404297\n",
      "Train loss: 2.178753614425659\n",
      "Train loss: 2.979750156402588\n",
      "Train loss: 2.3052587509155273\n",
      "Train loss: 2.1353940963745117\n",
      "Train loss: 2.2237746715545654\n",
      "Train loss: 2.4813523292541504\n",
      "Train loss: 2.3382625579833984\n",
      "Train loss: 3.8880648612976074\n",
      "Train loss: 2.1275641918182373\n",
      "Train loss: 3.025819778442383\n",
      "Train loss: 2.9115359783172607\n",
      "Train loss: 2.7754323482513428\n",
      "Train loss: 2.610438823699951\n",
      "Train loss: 2.8445613384246826\n",
      "Train loss: 2.3985393047332764\n",
      "Train loss: 2.415151357650757\n",
      "Train loss: 2.338869094848633\n",
      "Train loss: 2.3208487033843994\n",
      "Train loss: 2.684436798095703\n",
      "Train loss: 2.215038537979126\n",
      "Train loss: 2.961930990219116\n",
      "Train loss: 3.6945102214813232\n",
      "Train loss: 2.435328483581543\n",
      "Train loss: 2.1507647037506104\n",
      "Train loss: 3.9688453674316406\n",
      "Train loss: 2.1166188716888428\n",
      "Train loss: 2.3038485050201416\n",
      "Train loss: 2.2897908687591553\n",
      "Train loss: 2.285594940185547\n",
      "Train loss: 2.844810724258423\n",
      "Train loss: 2.829622507095337\n",
      "Train loss: 2.71236515045166\n",
      "Train loss: 3.03376841545105\n",
      "Train loss: 2.6916191577911377\n",
      "Train loss: 2.3994433879852295\n",
      "Train loss: 2.5281479358673096\n",
      "Train loss: 2.501847982406616\n",
      "Train loss: 2.5228302478790283\n",
      "Train loss: 3.886445999145508\n",
      "Train loss: 2.421764850616455\n",
      "Train loss: 2.5784664154052734\n",
      "Train loss: 2.4952261447906494\n",
      "Train loss: 2.038830518722534\n",
      "Train loss: 3.334834575653076\n",
      "Train loss: 2.6412343978881836\n",
      "Train loss: 2.3538060188293457\n",
      "Train loss: 2.4980411529541016\n",
      "Train loss: 2.202191114425659\n",
      "Train loss: 3.138808250427246\n",
      "Train loss: 2.2037720680236816\n",
      "Train loss: 2.406160354614258\n",
      "Train loss: 2.5067546367645264\n",
      "Train loss: 2.805025815963745\n",
      "Train loss: 2.85290265083313\n",
      "Train loss: 2.6276588439941406\n",
      "Train loss: 2.2972095012664795\n",
      "Train loss: 2.201589822769165\n",
      "Train loss: 3.161365509033203\n",
      "Train loss: 2.3233745098114014\n",
      "Train loss: 2.0421972274780273\n",
      "Train loss: 2.45120906829834\n",
      "Train loss: 3.0132782459259033\n",
      "Train loss: 2.409595012664795\n",
      "Train loss: 3.4641950130462646\n",
      "Train loss: 2.473240852355957\n",
      "Train loss: 3.1454808712005615\n",
      "Train loss: 2.6583242416381836\n",
      "Train loss: 2.5465281009674072\n",
      "Train loss: 2.255174398422241\n",
      "Train loss: 3.394558906555176\n",
      "Train loss: 2.507061004638672\n",
      "Train loss: 2.427614450454712\n",
      "Train loss: 2.2845993041992188\n",
      "Train loss: 2.226780652999878\n",
      "Train loss: 2.36177921295166\n",
      "Train loss: 2.354304075241089\n",
      "Train loss: 3.3097503185272217\n",
      "Train loss: 2.490506410598755\n",
      "Train loss: 2.1673102378845215\n",
      "Train loss: 2.487370491027832\n",
      "Train loss: 2.41719913482666\n",
      "Train loss: 2.6846160888671875\n",
      "Train loss: 2.2718346118927\n",
      "Train loss: 2.41988205909729\n",
      "Train loss: 2.269967794418335\n",
      "Train loss: 2.3347625732421875\n",
      "Train loss: 2.222299337387085\n",
      "Train loss: 2.4058289527893066\n",
      "Train loss: 2.8044769763946533\n",
      "Train loss: 2.482819080352783\n",
      "Train loss: 3.1212680339813232\n",
      "Train loss: 2.44144344329834\n",
      "Train loss: 2.399874210357666\n",
      "Train loss: 3.37469220161438\n",
      "Train loss: 2.534075975418091\n",
      "Train loss: 2.397094249725342\n",
      "Train loss: 3.5046908855438232\n",
      "Train loss: 3.23610782623291\n",
      "Train loss: 2.271972894668579\n",
      "Train loss: 2.555469036102295\n",
      "Train loss: 3.0532402992248535\n",
      "Train loss: 2.658215284347534\n",
      "Train loss: 3.1889660358428955\n",
      "Train loss: 2.5361971855163574\n",
      "Train loss: 2.4138095378875732\n",
      "Train loss: 2.6770973205566406\n",
      "Train loss: 2.559854507446289\n",
      "Train loss: 3.365391731262207\n",
      "Train loss: 2.2677161693573\n",
      "Train loss: 3.7806010246276855\n",
      "Train loss: 2.051931381225586\n",
      "Train loss: 2.321237325668335\n",
      "Train loss: 2.466139078140259\n",
      "Train loss: 3.225642204284668\n",
      "Train loss: 2.414285898208618\n",
      "Train loss: 2.6951308250427246\n",
      "Train loss: 2.561906099319458\n",
      "Train loss: 2.9165427684783936\n",
      "Train loss: 2.917940378189087\n",
      "Train loss: 2.7538137435913086\n",
      "Train loss: 2.6283810138702393\n",
      "Train loss: 2.232395887374878\n",
      "Train loss: 3.5326907634735107\n",
      "Train loss: 3.2359061241149902\n",
      "Train loss: 2.6908130645751953\n",
      "Train loss: 2.6727993488311768\n",
      "Train loss: 2.62058687210083\n",
      "Train loss: 3.2752246856689453\n",
      "Train loss: 2.9876480102539062\n",
      "Train loss: 3.009505271911621\n",
      "Train loss: 2.4883763790130615\n",
      "Train loss: 2.9923958778381348\n",
      "Train loss: 3.6135478019714355\n",
      "Train loss: 2.5430116653442383\n",
      "Train loss: 2.4456095695495605\n",
      "Train loss: 2.5845015048980713\n",
      "Train loss: 2.449322462081909\n",
      "Train loss: 2.693916082382202\n",
      "Train loss: 2.402379274368286\n",
      "Train loss: 2.7444519996643066\n",
      "Train loss: 2.0820765495300293\n",
      "Train loss: 2.5707414150238037\n",
      "Train loss: 2.5198752880096436\n",
      "Train loss: 3.3447046279907227\n",
      "Train loss: 2.4580819606781006\n",
      "Train loss: 2.5081470012664795\n",
      "Train loss: 2.8109922409057617\n",
      "Train loss: 3.29327392578125\n",
      "Train loss: 3.0819735527038574\n",
      "Train loss: 2.9977011680603027\n",
      "Train loss: 2.4164791107177734\n",
      "Train loss: 2.214401960372925\n",
      "Train loss: 3.43080997467041\n",
      "Train loss: 2.504483222961426\n",
      "Train loss: 2.4596540927886963\n",
      "Train loss: 2.8363029956817627\n",
      "Train loss: 2.447019100189209\n",
      "Train loss: 2.496148109436035\n",
      "Train loss: 2.581331253051758\n",
      "Train loss: 2.348564863204956\n",
      "Train loss: 2.6328914165496826\n",
      "Train loss: 2.2999308109283447\n",
      "Train loss: 2.4950008392333984\n",
      "Train loss: 2.8442046642303467\n",
      "Train loss: 3.050916910171509\n",
      "Train loss: 1.911996603012085\n",
      "Train loss: 2.438568115234375\n",
      "Train loss: 2.2021310329437256\n",
      "Train loss: 2.5304946899414062\n",
      "Train loss: 2.565197467803955\n",
      "Train loss: 2.6045799255371094\n",
      "Train loss: 2.790954113006592\n",
      "Train loss: 2.9246644973754883\n",
      "Train loss: 3.166688919067383\n",
      "Train loss: 3.1483354568481445\n",
      "Train loss: 2.3055500984191895\n",
      "Train loss: 2.780747890472412\n",
      "Train loss: 2.0038485527038574\n",
      "Train loss: 2.7802937030792236\n",
      "Train loss: 2.6146836280822754\n",
      "Train loss: 3.219165086746216\n",
      "Train loss: 2.8051650524139404\n",
      "Train loss: 2.3360824584960938\n",
      "Train loss: 2.550448417663574\n",
      "Train loss: 2.24273419380188\n",
      "Train loss: 2.682588577270508\n",
      "Train loss: 2.553138017654419\n",
      "Train loss: 2.37190580368042\n",
      "Train loss: 2.2074098587036133\n",
      "Train loss: 2.0851502418518066\n",
      "Train loss: 2.9428398609161377\n",
      "Train loss: 2.268796682357788\n",
      "Train loss: 2.526494026184082\n",
      "Train loss: 2.5716090202331543\n",
      "Train loss: 2.4948017597198486\n",
      "Train loss: 2.4573776721954346\n",
      "Train loss: 2.592184543609619\n",
      "Train loss: 2.8642358779907227\n",
      "Train loss: 2.1373817920684814\n",
      "Train loss: 2.4623117446899414\n",
      "Train loss: 2.345705509185791\n",
      "Train loss: 2.3315696716308594\n",
      "Train loss: 2.793785572052002\n",
      "Train loss: 3.389411211013794\n",
      "Train loss: 2.4875149726867676\n",
      "Train loss: 2.403003692626953\n",
      "Train loss: 2.675081968307495\n",
      "Train loss: 2.5636563301086426\n",
      "Train loss: 2.7648277282714844\n",
      "Train loss: 2.4886884689331055\n",
      "Train loss: 3.5776524543762207\n",
      "Train loss: 2.752354860305786\n",
      "Train loss: 2.265672445297241\n",
      "Train loss: 2.4649243354797363\n",
      "Train loss: 2.428039073944092\n",
      "Train loss: 2.320307493209839\n",
      "Train loss: 2.731356620788574\n",
      "Train loss: 2.0663504600524902\n",
      "Train loss: 2.4140965938568115\n",
      "Train loss: 2.2880194187164307\n",
      "Train loss: 2.4933125972747803\n",
      "Train loss: 3.38301420211792\n",
      "Train loss: 2.530579090118408\n",
      "Train loss: 2.209967613220215\n",
      "Train loss: 3.6986894607543945\n",
      "Train loss: 2.215007781982422\n",
      "Train loss: 2.5690977573394775\n",
      "Train loss: 2.611762762069702\n",
      "Train loss: 2.420144557952881\n",
      "Train loss: 2.373788595199585\n",
      "Train loss: 2.4791674613952637\n",
      "Train loss: 2.3521153926849365\n",
      "Train loss: 2.2769315242767334\n",
      "Train loss: 2.285708427429199\n",
      "Train loss: 3.04972767829895\n",
      "Train loss: 2.282956600189209\n",
      "Train loss: 2.913814067840576\n",
      "Train loss: 2.573733329772949\n",
      "Train loss: 2.630816698074341\n",
      "Train loss: 2.7299792766571045\n",
      "Train loss: 2.507018566131592\n",
      "Train loss: 2.535733938217163\n",
      "Train loss: 2.5545098781585693\n",
      "Train loss: 2.3630409240722656\n",
      "Train loss: 2.970054864883423\n",
      "Train loss: 2.5786073207855225\n",
      "Train loss: 2.6747028827667236\n",
      "Train loss: 2.539998769760132\n",
      "Train loss: 3.3820395469665527\n",
      "Train loss: 2.536897659301758\n",
      "Train loss: 2.4880523681640625\n",
      "Train loss: 2.0718791484832764\n",
      "Train loss: 2.2795159816741943\n",
      "Train loss: 2.7083675861358643\n",
      "Train loss: 2.8912951946258545\n",
      "Train loss: 4.895346164703369\n",
      "Train loss: 2.850529670715332\n",
      "Train loss: 3.184858560562134\n",
      "Train loss: 2.6243205070495605\n",
      "Train loss: 2.3676605224609375\n",
      "Train loss: 2.9070892333984375\n",
      "Train loss: 2.676312208175659\n",
      "Train loss: 2.449061632156372\n",
      "Train loss: 2.2003180980682373\n",
      "Train loss: 2.5531811714172363\n",
      "Train loss: 2.4447174072265625\n",
      "Train loss: 3.036283493041992\n",
      "Train loss: 3.041781425476074\n",
      "Train loss: 2.947172164916992\n",
      "Train loss: 2.527255058288574\n",
      "Train loss: 2.937955141067505\n",
      "Train loss: 2.8816497325897217\n",
      "Train loss: 2.673980236053467\n",
      "Train loss: 3.0967135429382324\n",
      "Train loss: 2.7370128631591797\n",
      "Train loss: 2.8340957164764404\n",
      "Train loss: 2.310433864593506\n",
      "Train loss: 3.3095502853393555\n",
      "Train loss: 2.4819188117980957\n",
      "Train loss: 2.289235830307007\n",
      "Train loss: 2.3266854286193848\n",
      "Train loss: 2.5515966415405273\n",
      "Train loss: 2.728436231613159\n",
      "Train loss: 2.4311912059783936\n",
      "Train loss: 2.797070026397705\n",
      "Train loss: 2.269021987915039\n",
      "Train loss: 2.1415345668792725\n",
      "Train loss: 2.956210136413574\n",
      "Train loss: 2.9011518955230713\n",
      "Train loss: 2.475761890411377\n",
      "Train loss: 3.0523056983947754\n",
      "Train loss: 1.9015512466430664\n",
      "Train loss: 2.063448905944824\n",
      "Train loss: 2.3258180618286133\n",
      "Train loss: 3.11543607711792\n",
      "Train loss: 2.4683425426483154\n",
      "Train loss: 2.304905414581299\n",
      "Train loss: 2.3884341716766357\n",
      "Train loss: 2.825490951538086\n",
      "Train loss: 2.3719522953033447\n",
      "Train loss: 2.623980760574341\n",
      "Train loss: 4.467632293701172\n",
      "Train loss: 2.1644866466522217\n",
      "Train loss: 2.4323935508728027\n",
      "Train loss: 2.3725996017456055\n",
      "Train loss: 2.3701725006103516\n",
      "Train loss: 2.011423349380493\n",
      "Train loss: 2.507167339324951\n",
      "Train loss: 2.2984728813171387\n",
      "Train loss: 2.3520936965942383\n",
      "Train loss: 2.591494560241699\n",
      "Train loss: 3.0370302200317383\n",
      "Train loss: 2.692182779312134\n",
      "Train loss: 3.1235318183898926\n",
      "Train loss: 2.9567365646362305\n",
      "Train loss: 2.1717746257781982\n",
      "Train loss: 2.3795695304870605\n",
      "Train loss: 2.5811264514923096\n",
      "Train loss: 2.662769079208374\n",
      "Train loss: 2.4492905139923096\n",
      "Train loss: 3.072528123855591\n",
      "Train loss: 2.701321840286255\n",
      "Train loss: 3.184527635574341\n",
      "Train loss: 2.553464412689209\n",
      "Train loss: 2.286709785461426\n",
      "Train loss: 2.3884787559509277\n",
      "Train loss: 2.524693727493286\n",
      "Train loss: 2.2609071731567383\n",
      "Train loss: 2.6282382011413574\n",
      "Train loss: 2.9008278846740723\n",
      "Train loss: 2.551558017730713\n",
      "Train loss: 2.3164632320404053\n",
      "Train loss: 2.2732155323028564\n",
      "Train loss: 2.3569936752319336\n",
      "Train loss: 2.3245580196380615\n",
      "Train loss: 2.3415842056274414\n",
      "Train loss: 2.923248052597046\n",
      "Train loss: 2.4291770458221436\n",
      "Train loss: 2.4199209213256836\n",
      "Train loss: 2.2763006687164307\n",
      "Train loss: 2.879265308380127\n",
      "Train loss: 3.2968101501464844\n",
      "Train loss: 2.2700626850128174\n",
      "Train loss: 2.6222877502441406\n",
      "Train loss: 2.7288362979888916\n",
      "Train loss: 2.200103282928467\n",
      "Train loss: 2.378140449523926\n",
      "Train loss: 2.296257495880127\n",
      "Train loss: 3.150968551635742\n",
      "Train loss: 3.3233916759490967\n",
      "Train loss: 2.377460241317749\n",
      "Train loss: 2.8573710918426514\n",
      "Train loss: 2.3165974617004395\n",
      "Train loss: 2.6542840003967285\n",
      "Train loss: 2.29968523979187\n",
      "Train loss: 2.2422001361846924\n",
      "Train loss: 2.485419511795044\n",
      "Train loss: 2.568605899810791\n",
      "Train loss: 2.401879072189331\n",
      "Train loss: 2.5805115699768066\n",
      "Train loss: 2.1487069129943848\n",
      "Train loss: 2.278813362121582\n",
      "Train loss: 3.4531290531158447\n",
      "Train loss: 2.643672227859497\n",
      "Train loss: 2.349621057510376\n",
      "Train loss: 2.1012091636657715\n",
      "Train loss: 2.836064338684082\n",
      "Train loss: 2.860008716583252\n",
      "Train loss: 3.585442304611206\n",
      "Train loss: 2.393284320831299\n",
      "Train loss: 3.8090932369232178\n",
      "Train loss: 2.5193934440612793\n",
      "Train loss: 2.1737306118011475\n",
      "Train loss: 2.625345468521118\n",
      "Train loss: 2.401930570602417\n",
      "Train loss: 2.1573238372802734\n",
      "Train loss: 2.8932600021362305\n",
      "Train loss: 2.0991098880767822\n",
      "Train loss: 2.8067314624786377\n",
      "Train loss: 2.4249110221862793\n",
      "Train loss: 2.7495157718658447\n",
      "Train loss: 2.2994844913482666\n",
      "Train loss: 2.6780154705047607\n",
      "Train loss: 2.584073781967163\n",
      "Train loss: 2.6831865310668945\n",
      "Train loss: 2.6723649501800537\n",
      "Train loss: 2.5236780643463135\n",
      "Train loss: 2.387082815170288\n",
      "Train loss: 2.5881433486938477\n",
      "Train loss: 3.0467660427093506\n",
      "Train loss: 2.616708278656006\n",
      "Train loss: 2.236792802810669\n",
      "Train loss: 2.401207447052002\n",
      "Train loss: 2.3785924911499023\n",
      "Train loss: 2.5032119750976562\n",
      "Train loss: 2.195366382598877\n",
      "Train loss: 2.7580201625823975\n",
      "Train loss: 2.461472988128662\n",
      "Train loss: 2.7287423610687256\n",
      "Train loss: 2.521230459213257\n",
      "Train loss: 2.374098539352417\n",
      "Train loss: 4.473574161529541\n",
      "Train loss: 3.0481204986572266\n",
      "Train loss: 2.467221736907959\n",
      "Train loss: 3.67362380027771\n",
      "Train loss: 3.098902463912964\n",
      "Train loss: 2.080487012863159\n",
      "Train loss: 2.885662078857422\n",
      "Train loss: 2.408491849899292\n",
      "Train loss: 2.1614205837249756\n",
      "Train loss: 2.186349630355835\n",
      "Train loss: 2.307048797607422\n",
      "Train loss: 3.2531590461730957\n",
      "Train loss: 2.497117280960083\n",
      "Train loss: 2.7793540954589844\n",
      "Train loss: 2.5883243083953857\n",
      "Train loss: 3.0965769290924072\n",
      "Train loss: 3.100484848022461\n",
      "Train loss: 2.318713665008545\n",
      "Train loss: 4.055662631988525\n",
      "Train loss: 1.9218740463256836\n",
      "Train loss: 2.312404155731201\n",
      "Train loss: 2.386265277862549\n",
      "Train loss: 3.026458501815796\n",
      "Train loss: 2.4985015392303467\n",
      "Train loss: 2.3432347774505615\n",
      "Train loss: 2.9561641216278076\n",
      "Train loss: 2.405022382736206\n",
      "Train loss: 2.593355178833008\n",
      "Train loss: 2.3927953243255615\n",
      "Train loss: 2.413186550140381\n",
      "Train loss: 2.3495595455169678\n",
      "Train loss: 2.2603702545166016\n",
      "Train loss: 2.152904510498047\n",
      "Train loss: 2.34297513961792\n",
      "Train loss: 3.2100675106048584\n",
      "Train loss: 3.630574941635132\n",
      "Train loss: 2.439014434814453\n",
      "Train loss: 3.3060691356658936\n",
      "Train loss: 2.3573198318481445\n",
      "Train loss: 2.794041872024536\n",
      "Train loss: 2.4829204082489014\n",
      "Train loss: 2.855475425720215\n",
      "Train loss: 2.5073764324188232\n",
      "Train loss: 2.178112506866455\n",
      "Train loss: 3.0392210483551025\n",
      "Train loss: 2.2782504558563232\n",
      "Train loss: 2.6716670989990234\n",
      "Train loss: 2.635937452316284\n",
      "Train loss: 2.362508535385132\n",
      "Train loss: 2.0612001419067383\n",
      "Train loss: 2.4973278045654297\n",
      "Train loss: 2.6095616817474365\n",
      "Train loss: 2.751690149307251\n",
      "Train loss: 2.8769447803497314\n",
      "Train loss: 2.322458028793335\n",
      "Train loss: 2.4766626358032227\n",
      "Train loss: 2.61348819732666\n",
      "Train loss: 2.7706804275512695\n",
      "Train loss: 1.9539778232574463\n",
      "Train loss: 2.566891670227051\n",
      "Train loss: 2.761225938796997\n",
      "Train loss: 2.748201370239258\n",
      "Train loss: 2.314453125\n",
      "Train loss: 2.5138442516326904\n",
      "Train loss: 2.581223726272583\n",
      "Train loss: 2.553802967071533\n",
      "Train loss: 2.3264663219451904\n",
      "Train loss: 2.9384078979492188\n",
      "Train loss: 2.3784046173095703\n",
      "Train loss: 2.428462028503418\n",
      "Train loss: 2.692355155944824\n",
      "Train loss: 2.554093360900879\n",
      "Train loss: 2.5039634704589844\n",
      "Train loss: 3.9562418460845947\n",
      "Train loss: 2.47723388671875\n",
      "Train loss: 2.7176215648651123\n",
      "Train loss: 2.4368813037872314\n",
      "Train loss: 2.4999477863311768\n",
      "Train loss: 3.4734532833099365\n",
      "Train loss: 2.627284049987793\n",
      "Train loss: 2.533158302307129\n",
      "Train loss: 2.3237791061401367\n",
      "Train loss: 2.7766242027282715\n",
      "Train loss: 2.5078682899475098\n",
      "Train loss: 2.902831554412842\n",
      "Train loss: 2.345085382461548\n",
      "Train loss: 2.6382503509521484\n",
      "Train loss: 2.5222008228302\n",
      "Train loss: 2.4194893836975098\n",
      "Train loss: 3.4822545051574707\n",
      "Train loss: 2.8778622150421143\n",
      "Train loss: 3.4153947830200195\n",
      "Train loss: 2.406013250350952\n",
      "Train loss: 3.2770090103149414\n",
      "Train loss: 2.1263890266418457\n",
      "Train loss: 3.0171475410461426\n",
      "Train loss: 2.0015869140625\n",
      "Train loss: 2.4848458766937256\n",
      "Train loss: 2.5398874282836914\n",
      "Train loss: 2.3136870861053467\n",
      "Train loss: 2.5002548694610596\n",
      "Train loss: 2.203857660293579\n",
      "Train loss: 2.3219552040100098\n",
      "Train loss: 2.565009832382202\n",
      "Train loss: 2.388798236846924\n",
      "Train loss: 3.505483865737915\n",
      "Train loss: 2.7614786624908447\n",
      "Train loss: 2.9126124382019043\n",
      "Train loss: 2.9365715980529785\n",
      "Train loss: 2.317202568054199\n",
      "Train loss: 2.3570075035095215\n",
      "Train loss: 2.6373422145843506\n",
      "Train loss: 2.360517740249634\n",
      "Train loss: 4.453230857849121\n",
      "Train loss: 2.7667102813720703\n",
      "Train loss: 3.1196136474609375\n",
      "Train loss: 2.6889114379882812\n",
      "Train loss: 2.4712400436401367\n",
      "Train loss: 2.8405539989471436\n",
      "Train loss: 3.358759880065918\n",
      "Train loss: 3.524299144744873\n",
      "Train loss: 3.252451181411743\n",
      "Train loss: 2.626490354537964\n",
      "Train loss: 2.231382369995117\n",
      "Train loss: 2.7712342739105225\n",
      "Train loss: 2.6006224155426025\n",
      "Train loss: 2.4585931301116943\n",
      "Train loss: 2.58780574798584\n",
      "Train loss: 2.183357000350952\n",
      "Train loss: 2.727648973464966\n",
      "Train loss: 2.380054473876953\n",
      "Train loss: 3.140566349029541\n",
      "Train loss: 2.467454671859741\n",
      "Train loss: 3.1594154834747314\n",
      "Train loss: 2.2138943672180176\n",
      "Train loss: 2.6278955936431885\n",
      "Train loss: 2.413780450820923\n",
      "Train loss: 2.525768995285034\n",
      "Train loss: 3.44738507270813\n",
      "Train loss: 2.5119469165802\n",
      "Train loss: 2.418959140777588\n",
      "Train loss: 2.21083664894104\n",
      "Train loss: 3.1122779846191406\n",
      "Train loss: 2.5916097164154053\n",
      "Train loss: 2.6118292808532715\n",
      "Train loss: 3.0584771633148193\n",
      "Train loss: 2.249509572982788\n",
      "Train loss: 2.148531436920166\n",
      "Train loss: 2.3996388912200928\n",
      "Train loss: 2.9744930267333984\n",
      "Train loss: 3.044809341430664\n",
      "Train loss: 2.5398142337799072\n",
      "Train loss: 2.253641128540039\n",
      "Train loss: 2.157604455947876\n",
      "Train loss: 2.1503489017486572\n",
      "Train loss: 2.076983690261841\n",
      "Train loss: 3.1857831478118896\n",
      "Train loss: 3.15106201171875\n",
      "Train loss: 2.1757030487060547\n",
      "Train loss: 2.739617109298706\n",
      "Train loss: 2.6547043323516846\n",
      "Train loss: 2.523522138595581\n",
      "Train loss: 2.462503433227539\n",
      "Train loss: 2.0980355739593506\n",
      "Train loss: 2.7015228271484375\n",
      "Train loss: 2.2454802989959717\n",
      "Train loss: 2.614664077758789\n",
      "Train loss: 2.976804494857788\n",
      "Train loss: 2.495321273803711\n",
      "Train loss: 3.1399643421173096\n",
      "Train loss: 2.8289225101470947\n",
      "Train loss: 2.5599827766418457\n",
      "Train loss: 2.715097427368164\n",
      "Train loss: 2.442988395690918\n",
      "Train loss: 2.7132177352905273\n",
      "Train loss: 2.653456211090088\n",
      "Train loss: 2.1045894622802734\n",
      "Train loss: 2.715498924255371\n",
      "Train loss: 3.940430164337158\n",
      "Train loss: 2.5089433193206787\n",
      "Train loss: 2.173053503036499\n",
      "Train loss: 3.039623498916626\n",
      "Train loss: 2.4981613159179688\n",
      "Train loss: 3.318868637084961\n",
      "Train loss: 2.50539493560791\n",
      "Train loss: 2.629373550415039\n",
      "Train loss: 2.2092747688293457\n",
      "Train loss: 2.7967751026153564\n",
      "Train loss: 2.897233486175537\n",
      "Train loss: 3.7398908138275146\n",
      "Train loss: 2.5601553916931152\n",
      "Train loss: 3.4481208324432373\n",
      "Train loss: 2.437505006790161\n",
      "Train loss: 2.5267844200134277\n",
      "Train loss: 2.4191062450408936\n",
      "Train loss: 3.0780372619628906\n",
      "Train loss: 3.132875442504883\n",
      "Train loss: 2.8999171257019043\n",
      "Train loss: 2.3932247161865234\n",
      "Train loss: 2.753079414367676\n",
      "Train loss: 2.7495858669281006\n",
      "Train loss: 2.455695152282715\n",
      "Train loss: 2.34997296333313\n",
      "Train loss: 3.302286148071289\n",
      "Train loss: 2.3476603031158447\n",
      "Train loss: 2.318910598754883\n",
      "Train loss: 3.5555925369262695\n",
      "Train loss: 2.7638633251190186\n",
      "Train loss: 2.3683066368103027\n",
      "Train loss: 2.193300247192383\n",
      "Train loss: 2.78340482711792\n",
      "Train loss: 3.0482234954833984\n",
      "Train loss: 2.926685333251953\n",
      "Train loss: 2.376307725906372\n",
      "Train loss: 2.309166193008423\n",
      "Train loss: 2.6048378944396973\n",
      "Train loss: 2.5838260650634766\n",
      "Train loss: 2.686445951461792\n",
      "Train loss: 2.26425838470459\n",
      "Train loss: 2.653425693511963\n",
      "Train loss: 2.046271800994873\n",
      "Train loss: 3.0713870525360107\n",
      "Train loss: 2.535562038421631\n",
      "Train loss: 2.901684284210205\n",
      "Train loss: 2.7651755809783936\n",
      "Train loss: 2.573382616043091\n",
      "Train loss: 2.5009560585021973\n",
      "Train loss: 3.647841215133667\n",
      "Train loss: 2.5047924518585205\n",
      "Train loss: 2.532247543334961\n",
      "Train loss: 2.2856717109680176\n",
      "Train loss: 2.5543346405029297\n",
      "Train loss: 2.152947425842285\n",
      "Train loss: 2.464207887649536\n",
      "Train loss: 3.2122130393981934\n",
      "Train loss: 2.28493070602417\n",
      "Train loss: 3.981605291366577\n",
      "Train loss: 2.4178853034973145\n",
      "Train loss: 2.620720624923706\n",
      "Train loss: 2.5036675930023193\n",
      "Train loss: 3.3597664833068848\n",
      "Train loss: 2.911823272705078\n",
      "Train loss: 3.084015130996704\n",
      "Train loss: 2.497218608856201\n",
      "Train loss: 2.805804491043091\n",
      "Train loss: 2.578348398208618\n",
      "Train loss: 3.235475540161133\n",
      "Train loss: 2.37762713432312\n",
      "Train loss: 2.5766124725341797\n",
      "Train loss: 2.6110076904296875\n",
      "Train loss: 2.340435743331909\n",
      "Train loss: 3.0465612411499023\n",
      "Train loss: 2.6444900035858154\n",
      "Train loss: 2.166980743408203\n",
      "Train loss: 2.186828136444092\n",
      "Train loss: 2.673839807510376\n",
      "Train loss: 3.0790202617645264\n",
      "Train loss: 3.254915952682495\n",
      "Train loss: 2.1441304683685303\n",
      "Train loss: 2.462810516357422\n",
      "Train loss: 2.5650899410247803\n",
      "Train loss: 3.152458667755127\n",
      "Train loss: 2.484174966812134\n",
      "Train loss: 2.409799814224243\n",
      "Train loss: 2.3456690311431885\n",
      "Train loss: 2.3660781383514404\n",
      "Train loss: 2.1612730026245117\n",
      "Train loss: 2.9139199256896973\n",
      "Train loss: 3.292915105819702\n",
      "Train loss: 2.9591386318206787\n",
      "Train loss: 2.697996139526367\n",
      "Train loss: 2.2339866161346436\n",
      "Train loss: 2.356860876083374\n",
      "Train loss: 2.418231964111328\n",
      "Train loss: 2.467973470687866\n",
      "Train loss: 2.518789529800415\n",
      "Train loss: 2.4581139087677\n",
      "Train loss: 2.6777048110961914\n",
      "Train loss: 2.6112706661224365\n",
      "Train loss: 2.427502155303955\n",
      "Train loss: 2.361839532852173\n",
      "Train loss: 3.5845558643341064\n",
      "Train loss: 3.1587510108947754\n",
      "Train loss: 2.0895981788635254\n",
      "Train loss: 2.1496071815490723\n",
      "Train loss: 3.6789116859436035\n",
      "Train loss: 2.0546090602874756\n",
      "Train loss: 2.4074082374572754\n",
      "Train loss: 2.332854747772217\n",
      "Train loss: 2.465017795562744\n",
      "Train loss: 2.715240716934204\n",
      "Train loss: 2.669949769973755\n",
      "Train loss: 2.2288947105407715\n",
      "Train loss: 2.3220362663269043\n",
      "Train loss: 2.5473737716674805\n",
      "Train loss: 2.568124532699585\n",
      "Train loss: 2.3262603282928467\n",
      "Train loss: 2.4578657150268555\n",
      "Train loss: 2.717681884765625\n",
      "Train loss: 2.607337713241577\n",
      "Train loss: 2.2525343894958496\n",
      "Train loss: 2.309096097946167\n",
      "Train loss: 2.513084650039673\n",
      "Train loss: 2.732837200164795\n",
      "Train loss: 3.2301580905914307\n",
      "Train loss: 2.884279727935791\n",
      "Train loss: 2.378093719482422\n",
      "Train loss: 3.1302895545959473\n",
      "Train loss: 2.803046941757202\n",
      "Train loss: 2.9374470710754395\n",
      "Train loss: 3.375013589859009\n",
      "Train loss: 2.333991289138794\n",
      "Train loss: 2.7175395488739014\n",
      "Train loss: 2.24503231048584\n",
      "Train loss: 2.248054027557373\n",
      "Train loss: 2.2758984565734863\n",
      "Train loss: 2.959207534790039\n",
      "Train loss: 2.311981678009033\n",
      "Train loss: 2.6751821041107178\n",
      "Train loss: 2.5815553665161133\n",
      "Train loss: 2.977869749069214\n",
      "Train loss: 2.470952033996582\n",
      "Train loss: 3.207463502883911\n",
      "Train loss: 2.863105535507202\n",
      "Train loss: 2.6546037197113037\n",
      "Train loss: 3.0557799339294434\n",
      "Train loss: 2.336405038833618\n",
      "Train loss: 2.3019633293151855\n",
      "Train loss: 2.8645825386047363\n",
      "Train loss: 2.4265379905700684\n",
      "Train loss: 2.238821268081665\n",
      "Train loss: 4.067318439483643\n",
      "Train loss: 3.2303924560546875\n",
      "Train loss: 2.4313242435455322\n",
      "Train loss: 2.7177624702453613\n",
      "Train loss: 2.449537754058838\n",
      "Train loss: 2.4489896297454834\n",
      "Train loss: 2.327124834060669\n",
      "Train loss: 1.9190776348114014\n",
      "Train loss: 2.700547695159912\n",
      "Train loss: 3.1313564777374268\n",
      "Train loss: 2.56728196144104\n",
      "Train loss: 3.67875337600708\n",
      "Train loss: 2.873878240585327\n",
      "Train loss: 2.1844449043273926\n",
      "Train loss: 2.3916914463043213\n",
      "Train loss: 2.8786373138427734\n",
      "Train loss: 2.5841667652130127\n",
      "Train loss: 2.3998241424560547\n",
      "Train loss: 2.5430994033813477\n",
      "Train loss: 2.62619686126709\n",
      "Train loss: 3.1355137825012207\n",
      "Train loss: 2.629193067550659\n",
      "Train loss: 2.638617753982544\n",
      "Train loss: 2.797562599182129\n",
      "Train loss: 2.35046124458313\n",
      "Train loss: 2.882884979248047\n",
      "Train loss: 2.4559335708618164\n",
      "Train loss: 2.994966506958008\n",
      "Train loss: 3.031594753265381\n",
      "Train loss: 2.2334139347076416\n",
      "Train loss: 2.60507869720459\n",
      "Train loss: 2.5320169925689697\n",
      "Train loss: 2.648554801940918\n",
      "Train loss: 2.9197518825531006\n",
      "Train loss: 2.423643112182617\n",
      "Train loss: 2.7725460529327393\n",
      "Train loss: 2.568171977996826\n",
      "Train loss: 2.666508674621582\n",
      "Train loss: 2.5658161640167236\n",
      "Train loss: 3.0648036003112793\n",
      "Train loss: 1.9086356163024902\n",
      "Train loss: 2.8457915782928467\n",
      "Train loss: 2.5664236545562744\n",
      "Train loss: 2.623779535293579\n",
      "Train loss: 2.383652925491333\n",
      "Train loss: 2.609916925430298\n",
      "Train loss: 2.4345993995666504\n",
      "Train loss: 2.4952046871185303\n",
      "Train loss: 3.014690399169922\n",
      "Train loss: 2.4688022136688232\n",
      "Train loss: 2.6246116161346436\n",
      "Train loss: 2.960124969482422\n",
      "Train loss: 2.3011374473571777\n",
      "Train loss: 2.7792909145355225\n",
      "Train loss: 2.7559401988983154\n",
      "Train loss: 2.2198431491851807\n",
      "Train loss: 2.7291135787963867\n",
      "Train loss: 2.9727470874786377\n",
      "Train loss: 2.1091561317443848\n",
      "Train loss: 3.250107765197754\n",
      "Train loss: 2.550886631011963\n",
      "Train loss: 2.343916893005371\n",
      "Train loss: 2.387558698654175\n",
      "Train loss: 3.319272041320801\n",
      "Train loss: 2.389324426651001\n",
      "Train loss: 2.47782826423645\n",
      "Train loss: 2.9013278484344482\n",
      "Train loss: 2.776732921600342\n",
      "Train loss: 2.500410318374634\n",
      "Train loss: 2.839919090270996\n",
      "Train loss: 2.1278183460235596\n",
      "Train loss: 2.2731199264526367\n",
      "Train loss: 2.3183112144470215\n",
      "Train loss: 2.3128654956817627\n",
      "Train loss: 2.803004741668701\n",
      "Train loss: 2.2610979080200195\n",
      "Train loss: 2.7615487575531006\n",
      "Train loss: 2.3811635971069336\n",
      "Train loss: 3.2525737285614014\n",
      "Train loss: 3.4972286224365234\n",
      "Train loss: 2.118408441543579\n",
      "Train loss: 2.6041958332061768\n",
      "Train loss: 3.112161874771118\n",
      "Train loss: 2.387691020965576\n",
      "Train loss: 2.7695083618164062\n",
      "Train loss: 2.148198366165161\n",
      "Train loss: 2.934636116027832\n",
      "Train loss: 3.2522029876708984\n",
      "Train loss: 2.520075798034668\n",
      "Train loss: 2.190262794494629\n",
      "Train loss: 2.1719634532928467\n",
      "Train loss: 2.192305326461792\n",
      "Train loss: 2.1893599033355713\n",
      "Train loss: 2.6229095458984375\n",
      "Train loss: 2.418694019317627\n",
      "Train loss: 2.409942626953125\n",
      "Train loss: 2.5755677223205566\n",
      "Train loss: 2.577972888946533\n",
      "Train loss: 2.423919200897217\n",
      "Train loss: 2.4524266719818115\n",
      "Train loss: 2.5821173191070557\n",
      "Train loss: 2.4900903701782227\n",
      "Train loss: 2.3878893852233887\n",
      "Train loss: 2.849440813064575\n",
      "Train loss: 2.90578031539917\n",
      "Train loss: 2.4819250106811523\n",
      "Train loss: 3.252056121826172\n",
      "Train loss: 3.330109119415283\n",
      "Train loss: 4.172182559967041\n",
      "Train loss: 2.8021979331970215\n",
      "Train loss: 2.129176378250122\n",
      "Train loss: 2.3023841381073\n",
      "Train loss: 2.378903388977051\n",
      "Train loss: 2.3327691555023193\n",
      "Train loss: 2.9106130599975586\n",
      "Train loss: 2.506197452545166\n",
      "Train loss: 2.3465564250946045\n",
      "Train loss: 2.022439956665039\n",
      "Train loss: 2.15556263923645\n",
      "Train loss: 2.639521837234497\n",
      "Train loss: 2.3916561603546143\n",
      "Train loss: 2.9961729049682617\n",
      "Train loss: 2.425455331802368\n",
      "Train loss: 2.651761531829834\n",
      "Train loss: 2.383845806121826\n",
      "Train loss: 2.3615927696228027\n",
      "Train loss: 2.3294389247894287\n",
      "Train loss: 2.499640941619873\n",
      "Train loss: 2.1490888595581055\n",
      "Train loss: 3.1512231826782227\n",
      "Train loss: 3.9204351902008057\n",
      "Train loss: 3.372450351715088\n",
      "Train loss: 2.754849672317505\n",
      "Train loss: 2.3959641456604004\n",
      "Train loss: 2.119596481323242\n",
      "Train loss: 2.440858840942383\n",
      "Train loss: 2.2310969829559326\n",
      "Train loss: 2.5753183364868164\n",
      "Train loss: 2.7905728816986084\n",
      "Train loss: 2.6649134159088135\n",
      "Train loss: 2.6736741065979004\n",
      "Train loss: 3.527263879776001\n",
      "Train loss: 2.2933878898620605\n",
      "Train loss: 2.0469586849212646\n",
      "Train loss: 2.558793067932129\n",
      "Train loss: 2.6633217334747314\n",
      "Train loss: 2.4717307090759277\n",
      "Train loss: 2.037651538848877\n",
      "Train loss: 2.720954656600952\n",
      "Train loss: 2.8651344776153564\n",
      "Train loss: 2.2467334270477295\n",
      "Train loss: 2.389582872390747\n",
      "Train loss: 2.402977466583252\n",
      "Train loss: 3.386369228363037\n",
      "Train loss: 2.6056172847747803\n",
      "Train loss: 2.2827308177948\n",
      "Train loss: 2.377678871154785\n",
      "Train loss: 2.505126953125\n",
      "Train loss: 2.318521022796631\n",
      "Train loss: 3.2074460983276367\n",
      "Train loss: 1.9180231094360352\n",
      "Train loss: 2.0940322875976562\n",
      "Train loss: 2.8872857093811035\n",
      "Train loss: 3.1227285861968994\n",
      "Train loss: 2.6081604957580566\n",
      "Train loss: 2.3533337116241455\n",
      "Train loss: 2.2184510231018066\n",
      "Train loss: 3.079315185546875\n",
      "Train loss: 2.7155590057373047\n",
      "Train loss: 3.256403684616089\n",
      "Train loss: 2.887610673904419\n",
      "Train loss: 2.591930627822876\n",
      "Train loss: 2.5063533782958984\n",
      "Train loss: 2.403501272201538\n",
      "Train loss: 2.1906490325927734\n",
      "Train loss: 3.1061084270477295\n",
      "Train loss: 2.9347712993621826\n",
      "Train loss: 2.68239688873291\n",
      "Train loss: 2.2122466564178467\n",
      "Train loss: 3.1292428970336914\n",
      "Train loss: 2.771287202835083\n",
      "Train loss: 3.3534469604492188\n",
      "Train loss: 2.1581215858459473\n",
      "Train loss: 2.982065439224243\n",
      "Train loss: 2.397141456604004\n",
      "Train loss: 2.2162976264953613\n",
      "Train loss: 2.7980129718780518\n",
      "Train loss: 2.5763256549835205\n",
      "Train loss: 2.951633930206299\n",
      "Train loss: 2.341493844985962\n",
      "Train loss: 2.5110297203063965\n",
      "Train loss: 2.757702350616455\n",
      "Train loss: 2.591540575027466\n",
      "Train loss: 2.3315341472625732\n",
      "Train loss: 2.328650951385498\n",
      "Train loss: 2.2172658443450928\n",
      "Train loss: 2.5591342449188232\n",
      "Train loss: 2.5442698001861572\n",
      "Train loss: 2.3692142963409424\n",
      "Train loss: 2.7801191806793213\n",
      "Train loss: 2.3579564094543457\n",
      "Train loss: 2.6759696006774902\n",
      "Train loss: 2.6166343688964844\n",
      "Train loss: 2.34454083442688\n",
      "Train loss: 3.441903829574585\n",
      "Train loss: 2.4848594665527344\n",
      "Train loss: 2.3126707077026367\n",
      "Train loss: 2.401789665222168\n",
      "Train loss: 2.4095122814178467\n",
      "Train loss: 3.220339298248291\n",
      "Train loss: 2.512810468673706\n",
      "Train loss: 2.9822466373443604\n",
      "Train loss: 2.8969554901123047\n",
      "Train loss: 3.4981210231781006\n",
      "Train loss: 2.5217928886413574\n",
      "Train loss: 2.3608124256134033\n",
      "Train loss: 2.4950966835021973\n",
      "Train loss: 3.5820751190185547\n",
      "Train loss: 2.151374340057373\n",
      "Train loss: 2.62644624710083\n",
      "Train loss: 2.206346035003662\n",
      "Train loss: 2.711703062057495\n",
      "Train loss: 2.824481964111328\n",
      "Train loss: 2.9212300777435303\n",
      "Train loss: 3.598219394683838\n",
      "Train loss: 2.91025447845459\n",
      "Train loss: 3.248206853866577\n",
      "Train loss: 2.7664401531219482\n",
      "Train loss: 2.4559414386749268\n",
      "Train loss: 2.1848597526550293\n",
      "Train loss: 2.5347797870635986\n",
      "Train loss: 2.5502333641052246\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "for iter in range(1000):\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Train loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Anousstem ale\n",
      "Gllere; and lenk t hararo I me he. than'dis, se ho r:\n",
      "Hor'the,\n",
      "\n",
      "\n",
      "Anchatlald tharematrt ny.\n",
      "MPrcau,\n",
      "Fove?\n",
      "\n",
      "HAnt minde ING nneendirove byoingld,\n",
      "\n",
      "L: we me e.\n",
      "Tovely tst ter ED:\n",
      "MEE f hird ivasavelo oth theme-thos med?\n",
      "Yo opotavo thovant atherus chessy h sathelors atwigre htou LTUENGUn w,\n",
      "Ye higllel m nghed ardr hisss;\n",
      "ARLAfithasoaso wie ye es ced, hersw hepr:\n",
      "Whistrd owie, d! oy thicon allapotaigice.\n",
      "CAnd f se theve rdat,\n",
      "\n",
      "IUTw'ste, marig arethoushourmefred, hery,\n",
      "\n",
      "hin'dwr: shint has g st e bose?\n",
      "\n",
      "Murd nd--mime, bun me\n",
      "I brr orthon tourowd I fowin:\n",
      "F Twh pes arshe\n",
      "AUShe gon\n",
      "\n",
      "LIAntuce pory lure my t; houtho aneanghatil bamppug beavey prere, e yolin,\n",
      "\n",
      "Peng the br;\n",
      "\n",
      "ANCEd tow wethemo nk,\n",
      "LLENG ppoit my theshoth ay barus whe hanowhod thes fioprtofenavis pllishaton thaus,\n",
      "Dof insthare?\n",
      "AGoowhen wesu wh urorent swono as thedengndscotr in m ther.\n",
      "ICl n mo oy s t hord wredo w wo\n",
      "\n",
      "HASTE ghes l sus manorng ounangnt mear brteeen' psthe, anced prs,\n",
      "RDo wotoon, Cangre, matedeacheramye g\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(torch.zeros((1, 1), dtype=torch.long), 1000)\n",
    "\n",
    "print(decode(output[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 2\n",
    "\n",
    "# Let's create a mock sequence of embedded tokens.\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "x.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive way to predict next token is by using the mean of all previous token's embeddings.\n",
    "\n",
    "$$ \\hat{x*t} = f(\\Sigma*{i=0}^{t-1} x_i) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 3.99 ms, total: 3.99 ms\n",
      "Wall time: 10.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Slow way, for-loop...\n",
    "\n",
    "xbow = torch.zeros((B, T, C))\n",
    "\n",
    "for b in range(B):  # For each sequence in batch.\n",
    "    for t in range(T):  # For each time step in seq.\n",
    "        xprev = x[b, : t + 1]\n",
    "        xbow[b, t] = torch.mean(xprev, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.2 ms, sys: 330 µs, total: 3.53 ms\n",
      "Wall time: 1.59 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Fast way - matrix mult!\n",
    "\n",
    "wei = torch.tril(torch.ones(T, T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
