{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cells below were created as notes while watching Andrej Karpathy's video \"[Let's build GPT: from scratch, in code, spelled out.\n",
    "](https://www.youtube.com/watch?v=kCc8FmEb1nY)\". Check it out if you haven't!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens - encoding text to a numeric format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 50257\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# This is the tokenizer used by GPT-2.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', ' rather', ' long', ' text', ' to', ' demonstrate', ' the', ' token', 'izer', '.', ' Co', 'ool', ',', ' right', '?']\n",
      "[32, 2138, 890, 2420, 284, 10176, 262, 11241, 7509, 13, 1766, 970, 11, 826, 30]\n"
     ]
    }
   ],
   "source": [
    "test_str = \"A rather long text to demonstrate the tokenizer. Coool, right?\"\n",
    "\n",
    "# GPT-2 used a subword tokenizer, meaning that each token corresponds to part of a word\n",
    "str_enc = tokenizer.encode(test_str)  # Tokenized string\n",
    "print([tokenizer.decode([s]) for s in str_enc])\n",
    "print(str_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 10\n",
      "Length of encoded seq: 15\n"
     ]
    }
   ],
   "source": [
    "# Note that tokens correspond to subwords. Because of this, the encoded sequence has more tokens compared to the number of words in the encoded text.\n",
    "print(f\"Length of text: {len(test_str.split(' '))}\")\n",
    "print(f\"Length of encoded seq: {len(str_enc)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will be using text from Shakespeare as our corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a very simple tokenization scheme - encoding single characters as tokens.\n",
    "\n",
    "Therefore, our vocabulary will consist of all symbols used in the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "print(\"\".join(vocab))\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our vocabulary is much smaller compared to the GPT-2 tokenizer. Keep this in mind as we continue!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple tokenization scheme by using the character's index in the vocabulary as its token.\n",
    "stoi = {ch: i for i, ch in enumerate(vocab)}  # string-to-integer\n",
    "itos = {i: ch for i, ch in enumerate(vocab)}  # integer-to-string\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([itos[i] for i in l])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's encode our original example text again, now using this simple tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 1, 56, 39, 58, 46, 43, 56, 1, 50, 53, 52, 45, 1, 58, 43, 62, 58, 1, 58, 53, 1, 42, 43, 51, 53, 52, 57, 58, 56, 39, 58, 43, 1, 58, 46, 43, 1, 58, 53, 49, 43, 52, 47, 64, 43, 56, 8, 1, 15, 53, 53, 53, 50, 6, 1, 56, 47, 45, 46, 58, 12]\n",
      "A rather long text to demonstrate the tokenizer. Coool, right?\n"
     ]
    }
   ],
   "source": [
    "print(encode(test_str))\n",
    "print(decode(encode(test_str)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how our encoded sequence compares to the original text in length now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 10\n",
      "Length of encoded seq: 62\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of text: {len(test_str.split(' '))}\")\n",
    "print(f\"Length of encoded seq: {len(encode(test_str))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how it is much longer? Since we are using a smaller vocabulary, we must use more tokens to encode our sequences.\n",
    "\n",
    "This shows the inherent relationship between vocabulary size and sequence length - a smaller vocabulary results in longer sequences.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WIP - Predicting the next token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 2\n",
    "\n",
    "# Let's create a mock sequence of embedded tokens.\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive way to predict next token is by using the mean of all previous token's embeddings.\n",
    "\n",
    "$$ \\hat{x_t} = f(\\Sigma_{i=0}^{t-1} x_i) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.08 ms, sys: 788 µs, total: 1.87 ms\n",
      "Wall time: 1.25 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Slow way, for-loop...\n",
    "\n",
    "xbow = torch.zeros((B, T, C))\n",
    "\n",
    "for b in range(B):  # For each sequence in batch.\n",
    "    for t in range(T):  # For each time step in seq.\n",
    "        xprev = x[b, : t + 1]\n",
    "        xbow[b, t] = torch.mean(xprev, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.47 ms, sys: 58 µs, total: 4.52 ms\n",
      "Wall time: 2.12 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Fast way - matrix mult!\n",
    "\n",
    "wei = torch.tril(torch.ones(T, T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
