{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cells below were created as notes while watching Andrej Karpathy's video \"[Let's build GPT: from scratch, in code, spelled out.\n",
    "](https://www.youtube.com/watch?v=kCc8FmEb1nY)\". Check it out if you haven't!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens - encoding text to a numeric format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 50257\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# This is the tokenizer used by GPT-2.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', ' rather', ' long', ' text', ' to', ' demonstrate', ' the', ' token', 'izer', '.', ' Co', 'ool', ',', ' right', '?']\n",
      "[32, 2138, 890, 2420, 284, 10176, 262, 11241, 7509, 13, 1766, 970, 11, 826, 30]\n"
     ]
    }
   ],
   "source": [
    "test_str = \"A rather long text to demonstrate the tokenizer. Coool, right?\"\n",
    "\n",
    "# GPT-2 used a subword tokenizer, meaning that each token corresponds to part of a word\n",
    "str_enc = tokenizer.encode(test_str)  # Tokenized string\n",
    "print([tokenizer.decode([s]) for s in str_enc])\n",
    "print(str_enc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 10\n",
      "Length of encoded seq: 15\n"
     ]
    }
   ],
   "source": [
    "# Note that tokens correspond to subwords. Because of this, the encoded sequence has more tokens compared to the number of words in the encoded text.\n",
    "print(f\"Length of text: {len(test_str.split(' '))}\")\n",
    "print(f\"Length of encoded seq: {len(str_enc)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will be using text from Shakespeare as our corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:300])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a very simple tokenization scheme - encoding single characters as tokens.\n",
    "\n",
    "Therefore, our vocabulary will consist of all symbols used in the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "print(\"\".join(vocab))\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size: {vocab_size}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our vocabulary is much smaller compared to the GPT-2 tokenizer. Keep this in mind as we continue!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple tokenization scheme by using the character's index in the vocabulary as its token.\n",
    "stoi = {ch: i for i, ch in enumerate(vocab)}  # string-to-integer\n",
    "itos = {i: ch for i, ch in enumerate(vocab)}  # integer-to-string\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([itos[i] for i in l])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's encode our original example text again, now using this simple tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 1, 56, 39, 58, 46, 43, 56, 1, 50, 53, 52, 45, 1, 58, 43, 62, 58, 1, 58, 53, 1, 42, 43, 51, 53, 52, 57, 58, 56, 39, 58, 43, 1, 58, 46, 43, 1, 58, 53, 49, 43, 52, 47, 64, 43, 56, 8, 1, 15, 53, 53, 53, 50, 6, 1, 56, 47, 45, 46, 58, 12]\n",
      "A rather long text to demonstrate the tokenizer. Coool, right?\n"
     ]
    }
   ],
   "source": [
    "print(encode(test_str))\n",
    "print(decode(encode(test_str)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how our encoded sequence compares to the original text in length now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 10\n",
      "Length of encoded seq: 62\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of text: {len(test_str.split(' '))}\")\n",
    "print(f\"Length of encoded seq: {len(encode(test_str))}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how it is much longer? Since we are using a smaller vocabulary, we must use more tokens to encode our sequences.\n",
    "\n",
    "This shows the inherent relationship between vocabulary size and sequence length - a smaller vocabulary results in longer sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[: block_size + 1]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram model\n",
    "\n",
    "Let's build arguably the simplest language model possible - a bigram model, which predicts the next token based on the previous token only. This means that we are modeling our text as Markov process, where the probability of the next state (token) only depends on the present state (the previous token),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.embedding(idx)\n",
    "\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits_ = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits_, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, num_steps):\n",
    "        for _ in range(num_steps):\n",
    "            logits, _ = self.forward(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            # Note: Sampling is probabilistic.\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 4.784310817718506\n",
      "Train loss: 3.5806233882904053\n",
      "Train loss: 2.2971673011779785\n",
      "Train loss: 2.68766450881958\n",
      "Train loss: 2.5791971683502197\n",
      "Train loss: 2.551861524581909\n",
      "Train loss: 2.7095766067504883\n",
      "Train loss: 3.2053205966949463\n",
      "Train loss: 2.8106496334075928\n",
      "Train loss: 2.5617117881774902\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "for iter in range(10000):\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if iter % 1000 == 0:\n",
    "        print(f\"Train loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "y me the,\n",
      "And\n",
      "BEE:\n",
      "\n",
      "Corear car hyothed:\n",
      "NETI:\n",
      "TI\n",
      "A alury cerenend,\n",
      "INAf; winde g wald slul bun whe\n",
      "\n",
      "K:\n",
      "Houker\n",
      "Byo he kee ty,\n",
      "\n",
      "IEThout st:\n",
      "\n",
      "\n",
      "G sayomatondgrende'TI'lanthinin: ans nd s\n",
      "\n",
      "He re.\n",
      "LUShoree, somarn I's temy kethiersptersureyond myou ftr for g o walos nt kit on d.\n",
      "\n",
      "Bulondstyonse indis prentous oodod.\n",
      "toupreaco llet if tht\n",
      "\n",
      "Thteais;\n",
      "Malesheais whe; linon p nde yomim d n,\n",
      "TIVI poowour w h nig har-----rin lis s ar:\n",
      "Gereitho wond bu ack'\n",
      "G nghat held inghe beromeas,\n",
      "ARE y as.\n",
      "Wht fe, heale fon:\n",
      "Berernkit stoncet mollar dir ce s:\n",
      "\n",
      "Fam OLALElodo the horerthe IOPrure se,\n",
      "LI bor d,\n",
      "\n",
      "CUSwamadr t k is chaverofithanontowng boute wantorr ithe,\n",
      "The l what worathalichor f shecthipr dowserll! cat be he oong st,\n",
      "Upl, ifrenit w fo swooul.\n",
      "US ond, it le litene,\n",
      "II bur e Win'soursce kelind seail thit:\n",
      "War hee pend; YOLUE:\n",
      "\n",
      "\n",
      "As, aurntated ay l barfacee hes ndetasuse\n",
      "\n",
      "IIONGLLInthe Wie tan dan mbe, mang sue u wengesitor wis.\n",
      "Fo dat\n",
      "IUS:\n",
      "ANUK:\n",
      "Fathtethemyor owelen.\n",
      "I mikece;\n",
      "LALom hat beexcer ingr we\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(torch.zeros((1, 1), dtype=torch.long), 1000)\n",
    "\n",
    "print(decode(output[0].tolist()))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not that bad, but far from perfect.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive way to predict next token is by using the mean of all previous token's embeddings.\n",
    "\n",
    "$$ \\hat{x*t} = f(\\Sigma*{i=0}^{t-1} x_i) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 4])\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [4., 4., 4., 4.],\n",
      "        [5., 5., 5., 5.],\n",
      "        [6., 6., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "T, C = 6, 4\n",
    "\n",
    "# Let's create a mock sequence of embedded tokens. Let's give each row incrementing values to more easily see what happens.\n",
    "x = torch.arange(1, T + 1).view(-1, 1).repeat(1, C).float()\n",
    "\n",
    "print(x.shape)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.5000, 1.5000, 1.5000, 1.5000],\n",
      "        [2.0000, 2.0000, 2.0000, 2.0000],\n",
      "        [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "        [3.0000, 3.0000, 3.0000, 3.0000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000]])\n"
     ]
    }
   ],
   "source": [
    "# Slow way, for-loop...\n",
    "\n",
    "\n",
    "def mean(x):\n",
    "    xbow = torch.zeros((T, C))\n",
    "\n",
    "    for t in range(T):  # For each time step in seq.\n",
    "        xprev = x[: t + 1, :]\n",
    "        xbow[t] = torch.mean(xprev, dim=0)\n",
    "\n",
    "    return xbow\n",
    "\n",
    "\n",
    "print(mean(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Fast way - matrix mult!\n",
    "\n",
    "wei = torch.tril(torch.ones(T, T))  # Create lower triangular matrix.\n",
    "print(wei)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.,  1.],\n",
      "        [ 3.,  3.,  3.,  3.],\n",
      "        [ 6.,  6.,  6.,  6.],\n",
      "        [10., 10., 10., 10.],\n",
      "        [15., 15., 15., 15.],\n",
      "        [21., 21., 21., 21.]])\n"
     ]
    }
   ],
   "source": [
    "# When matmuled with sequence with shape (T, C), we almost get what we want - we get the sum of the rows, but not the mean.\n",
    "print(wei @ x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.5000, 1.5000, 1.5000, 1.5000],\n",
      "        [2.0000, 2.0000, 2.0000, 2.0000],\n",
      "        [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "        [3.0000, 3.0000, 3.0000, 3.0000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000]])\n"
     ]
    }
   ],
   "source": [
    "# To get the mean instead, we normalize each row to sum to 1.\n",
    "row_norm = wei.sum(dim=1, keepdim=True)\n",
    "wei = wei / row_norm\n",
    "\n",
    "print(wei @ x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare speeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15 µs ± 53 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "wei @ x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.6 µs ± 1.5 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "mean(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than 10x faster, and we are not even using the GPU - not bad\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there is one more way to achieve the same outcome using softmax across each row, which normalizes it to sum to 1. To mask out future tokens, we replace their values by -inf first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]])\n",
      "tensor([[1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.5000, 1.5000, 1.5000, 1.5000],\n",
      "        [2.0000, 2.0000, 2.0000, 2.0000],\n",
      "        [2.5000, 2.5000, 2.5000, 2.5000],\n",
      "        [3.0000, 3.0000, 3.0000, 3.0000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000]])\n"
     ]
    }
   ],
   "source": [
    "tri = torch.tril(torch.ones(T, T))\n",
    "\n",
    "wei = torch.ones((T, T))\n",
    "wei = wei.masked_fill(tri == 0, -torch.inf)\n",
    "wei = F.softmax(wei, dim=1)\n",
    "\n",
    "print(wei)\n",
    "print(wei @ x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason this is interesting is because this approach works regardless of the initial values of the weights matrix.\n",
    "\n",
    "We can fill it with random values and still get rows that are normalized to 1:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.7089, 0.2911, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5166, 0.3210, 0.1624, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1616, 0.3187, 0.1423, 0.3774, 0.0000, 0.0000],\n",
       "        [0.3024, 0.1149, 0.0874, 0.4666, 0.0287, 0.0000],\n",
       "        [0.1292, 0.1173, 0.1132, 0.0494, 0.1582, 0.4326]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.randn((T, T))\n",
    "wei = wei.masked_fill(tri == 0, -torch.inf)\n",
    "wei = F.softmax(wei, dim=1)\n",
    "\n",
    "wei\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Describe how this can be intepreted as affinities, or coupling, between tokens.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-attention\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, can we do better than a simple average of the previous tokens? Indeed we can! We can do a weighted sum, where the weights represent the importance of each token. Even better, we can do a _learned_ weighted sum, where the weights are data-dependent.\n",
    "\n",
    "This is in fact exactly what the mechanism underlying transformers does, so called _self-attention_. It works like this:\n",
    "\n",
    "1. Each token emits three values: 1) a query _Q_, telling other tokens what it is \"looking for\", 2) a key _K_, telling other tokens what its own identity, and 3) a value _V_, which is the value it emits if interacted with. These are all outputs of learned functions, typically `nn.Linear` layers.\n",
    "2. Every token's query is multiplied through a dot-product with every other token's key, giving the _attention weights_ alpha\n",
    "3. The attention weights are used to do a weighted sum of the values to produce the output features\n",
    "\n",
    "The image below shows a single token's query interacting with the keys and values of the other tokens:\n",
    "\n",
    "![Alt text](./images/attention_example.svg)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WIP Describe scaled dot-product attention\n",
    "\n",
    "![Alt text](./images/scaled_dot_product_attn.svg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0255, 0.9745, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1819, 0.8151, 0.0030, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0498, 0.0073, 0.9410, 0.0018, 0.0000, 0.0000],\n",
      "        [0.0122, 0.2620, 0.0863, 0.0065, 0.6330, 0.0000],\n",
      "        [0.1704, 0.1016, 0.0054, 0.6455, 0.0112, 0.0659]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-0.2413,  0.0150,  0.1314,  0.8664, -0.0139, -0.2534, -0.3954, -0.1627,\n",
      "         -0.1547, -0.1018, -0.0184,  0.5015, -0.3924,  0.1592,  0.6902, -0.7130,\n",
      "          0.1581,  0.5846, -0.6353, -0.4605,  0.2287, -0.1649, -0.1271, -0.3449,\n",
      "         -0.1098, -0.3847,  0.7201, -0.0903, -0.2255, -0.1913, -0.2528,  0.2243],\n",
      "        [-1.1017, -0.3192, -0.5417,  1.0453,  0.6213, -0.1645,  0.2073, -0.4482,\n",
      "         -1.4633, -0.5932,  0.7303, -0.0174, -0.8551,  0.5447, -0.9249, -0.9371,\n",
      "         -0.6455,  0.8439, -0.1429,  0.3908, -0.1980, -0.4642, -1.0998, -0.7725,\n",
      "          0.0647,  0.4937, -0.5857, -0.6920, -0.5112,  0.8728, -0.2551,  0.1957],\n",
      "        [-0.9629, -0.2619, -0.4322,  1.0161,  0.5170, -0.1769,  0.1074, -0.3983,\n",
      "         -1.2492, -0.5098,  0.6060,  0.0659, -0.7782,  0.4807, -0.6617, -0.8991,\n",
      "         -0.5136,  0.8005, -0.2201,  0.2546, -0.1290, -0.4150, -0.9404, -0.7014,\n",
      "          0.0401,  0.3505, -0.3724, -0.5935, -0.4631,  0.7018, -0.2551,  0.2008],\n",
      "        [-0.8718,  0.8581, -0.0516,  0.8677, -0.1473,  0.4209, -0.8078,  0.8317,\n",
      "         -0.1337,  0.8622, -0.6026,  0.0199, -0.0217, -0.1305,  0.3972, -0.3002,\n",
      "          0.3140,  0.2652,  0.4107,  0.5008, -0.0268, -0.0956, -0.0175,  0.0215,\n",
      "          1.1294, -0.2495,  0.6331, -0.0531,  0.2036,  0.7614, -0.3639,  0.3794],\n",
      "        [-0.9018, -0.3017, -0.4935,  0.5648,  0.5752,  0.1478, -0.1287, -0.0708,\n",
      "         -1.3159, -0.4361,  0.3659, -0.0949, -0.4378,  0.5305, -1.0080, -0.5365,\n",
      "         -0.4781,  0.3983,  0.0338,  0.3442, -0.5172, -0.0235, -0.6552, -0.5286,\n",
      "          0.3911,  0.4723, -0.6791, -0.3780, -0.1297,  0.5983,  0.0622,  0.5347],\n",
      "        [ 0.3804, -0.8182,  0.2497,  0.3709,  0.1473, -0.4931, -0.2146, -0.6991,\n",
      "         -0.2821, -0.9016,  0.1948,  0.7835, -0.3308,  0.5165,  0.5152, -0.7026,\n",
      "          0.0299,  0.4599, -1.2309, -1.1852,  0.0212,  0.2081,  0.0755, -0.4039,\n",
      "         -0.7910, -0.3106,  0.4102,  0.1771, -0.2664, -1.1390,  0.2278,  0.4357]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "head_size = 32\n",
    "\n",
    "x = torch.randn(T, C)\n",
    "\n",
    "key = nn.Linear(C, head_size)\n",
    "query = nn.Linear(C, head_size)\n",
    "value = nn.Linear(C, head_size)\n",
    "\n",
    "k = key(x)  # (T, head_size)\n",
    "q = query(x)  # (T, head_size)\n",
    "v = value(x)  # (T, head_size)\n",
    "\n",
    "wei = q @ k.T  # (T, head_size) @ (head_size, T) = (T, T)\n",
    "wei = wei.masked_fill(tri == 0, -torch.inf)\n",
    "wei = F.softmax(wei, dim=1)\n",
    "\n",
    "print(wei)\n",
    "print(wei @ v)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer implementation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
